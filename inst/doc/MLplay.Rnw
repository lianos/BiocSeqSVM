%\VignetteIndexEntry{Maching learning from NGS data}
%\VignetteKeywords{tutorial}
%\VignettePackage{MLplay}
\documentclass[11pt]{article}

%% \usepackage{url}
\usepackage{hyperref}

\usepackage{colortbl}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{color}
\usepackage{xspace}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[boxed, linesnumbered]{algorithm2e}
\usepackage[
         colorlinks=true,
         linkcolor=blue,
         citecolor=blue,
         urlcolor=blue]
         {hyperref}
\usepackage{lscape}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Try to make things look purty
%% I like to use the fullpage!
\usepackage{fullpage}

%% Do not indent paragraphs
\usepackage[parfill]{parskip}

%% And fonts!
% For bold and small caps together
% http://stackoverflow.com/questions/699371/latex-small-caps-and-bold-face
\usepackage[T1]{fontenc}

%% Use Helvetica
\usepackage{times}
\usepackage[scaled=1]{helvet}     % PostScript font Helvetica for sans serif
\renewcommand{\rmdefault}{phv}    % Helvetica for roman type as well as sf
\renewcommand{\sfdefault}{phv}    % Helvetica for roman type as well as sf

% Inconsolata for monospaced fonts
% (fortunately this is distributed with the TeX-live distribution)
\usepackage{inconsolata}

%% End: Purty -----------------------------------------------------------------

\usepackage{Sweave}
\SweaveOpts{keep.source=TRUE}

% define new colors for use
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darkred}{rgb}{0.6,0.0,0}
\definecolor{lightbrown}{rgb}{1,0.9,0.8}
\definecolor{brown}{rgb}{0.6,0.3,0.3}
\definecolor{darkblue}{rgb}{0,0,0.8}
\definecolor{darkmagenta}{rgb}{0.5,0,0.5}

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}
\newcommand{\Rcode}[1]{{\texttt{#1}}}
\newcommand{\software}[1]{\textsf{#1}}
\newcommand{\R}{\software{R}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}

%% Either
% \lhead{}
% \chead{Machine Learning and NGS data}
% \rhead{}
% \lfoot{}
% \cfoot{}
% \rfoot{\thepage\ of \pageref{LastPage}}
% \renewcommand{\headrulewidth}{1pt}
% \renewcommand{\footrulewidth}{1pt}

%% Or
\fancyhead[RO,RE]{\slshape \leftmark}
% Using fancy headers sometimes lets figures, etc. bleed into the header, this
% trick has fixed it for me in the past
\setlength\headsep{25pt}

\title{Exploring the preferred binding landscape of an RNA binding protein using high throughput sequencing data and machine learning}

\author{
  Steve Lianoglou \\
  Memorial Sloan-Kettering Cancer Center \\
  \small{\texttt{lianos@cbio.mskcc.org}}
}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page

\maketitle

The goal of this tutorial is to provide a brief and intuitive introduction to
some machine learning techniques --- primarily support vector machines. Some
mathematical rigor will likely be sacrificed in order to appeal to intuition.
We will ultimately show how they can be used to explore new aspects of biology
that we can now assay for using high throughput sequencing data.

This vignette is written in a style that is meant to be amenable to self study.
A live presentation of this material should probably be less verbose.

\renewcommand{\baselinestretch}{.6}

\tableofcontents

\thispagestyle{empty}

\vspace{.2in}

\renewcommand{\baselinestretch}{1}

<<loadLibs, results=hide, echo=TRUE, eval=TRUE>>=
suppressPackageStartupMessages({
  library(Biostrings)
  library(shikken)
  library(fields)
})
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction
\section{Introduction}
\label{sec:introduction}

Machine learning (ML) methods play an increasingly important role in the fields
of biology and bioinformatics. Perhaps the largest deluge of the application of
ML techniques to biology happened after the widespread adoption of microarrays.
Microarrays make it easy for researchers to generate extremely high dimensional
datasets produced by measuring the expression of thousands of genes at a single
time point --- their interpretation, however, is often a challenge, and this is
where the smart application of some ML techniques can really pay off.

%% ============================================================================
%% Caveat emptor
\subsection{Caveat emptor}
\label{sub:caveat_emptor}
This tutorial is not meant to serve as a general purpose introduction to
machine learning concepts. Other tutorials are available that you can find online
that can serve this purpose. In particular, the ML lab and slides prepared by
Vincent Carey for the
\href{http://www.bioconductor.org/help/course-materials/2011/CSAMA/}{CSAMA 2011 workshop}
are worth exploring.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Support Vector Machines
\section{Support Vector Machines}
\label{sec:support_vector_machines}

The Support Vector Machine (SVM) is a versatile and state-of-the-art classifier
that was first introduced by Boser, Guyon and Vapnik~\cite{Boser:1992uo}. As
compared to other types of classifiers, an advantage the SVM is that it builds
a maximal margin (linear) classifiers that live in (extremely) high dimensional
spaces. SVMs can use kernels to exploit the relationship between the features
in your data in order to project it into a different (typically higher
dimensional) space in order to find a separating hyperplane to split the data.

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=0.5]{figs/decision-boundaries.pdf}
  \caption{
    Support vector machines find maximal margin separating hyperplanes.
    The red, green, and blue decision boundaries in \textbf{A} all separate
    the data equally well. The decision boundary found by the SVM in \textbf{B}
    is defined by the direction and magnitude of its orthonormal vector $\vec{W}$.
    This decision boundary is equally distant from the closest positive (filled) and
    negative (hollow) examples. The examples with dark borders are the support
    vectors, which orient the direction of $\vec{W}$. The red support vector is
    allowed to cross the margin, but pays a ``penalty'' $\xi_i$.
  }
  \label{fig:svmdecision}
\end{figure}

The idea of the maximum margin is shown in~Figure\ref{fig:svmdecision}.

The SVM optimizes the following (primal) loss equation:

\begin{equation}
  \min_{ {\bf w}, b} \max_{\alpha} \left\{ \frac{1}{2} \| {\bf w} \|^2 - \sum_{i=1}^n \alpha_i [ y_i ({\bf w} \cdot {\bf x}_i - b) - 1] \right\}
\label{eqn:primal}
\end{equation}

One particular drawback of SVMs is that it even though they can
produce highly accurate classification machines, the interpretation of the
classifiers you can build can prove difficult. We will discuss some ways
to open the ``black box'' of your classifier in this vignette as well.


%%to SVMs
%%   - Show primal
%%   - Show image of what it means to be a large margin classifier
%%   - Mention kernel trick
%%     This let's us measure differences between samples in different feature
%%     spaces more naturally.


%% ============================================================================
%% SVMs in R
\subsection{SVMs in R}
\label{sub:svms_in_r}

The are several interfaces to SVMs that already exist in \code{R}, namely
the \href{http://cran.r-project.org/web/packages/kernlab/}{kernlab}
and \code{http://cran.r-project.org/web/packages/e1071}{e1071} packages.
This tutorial, however, will introduce a new SVM package named
\href{http://lianos.github.com/shikken/}{shikken} which is a ``friendly''
wrapper to the
\href{http://www.shogun-toolbox.org/}{Shogun machine learning toolbox} --- a
\code{C++} library focussed on large scale kernel methods (especially
SVMs). We choose to use Shogun because it has a variety of string kernels
implemented in a memory and speed efficient manner, which will prove
useful in a variety of contexts that may arise when attempting to learn form
sequencing in your own research.

%% ============================================================================
%% Kernels
\subsection{Kernels}
\label{sub:kernels}

Kernels project data into different spaces.

<<label=easyClassificationProblem>>=
set.seed(123)
N <- 50

## easy to separate data
X1 <- matrix(c(rep(2, N) + rnorm(N, 0.5), 1 + rnorm(N)), ncol=2)
X2 <- matrix(c(rep(-2, N) - rnorm(N, 0.5), rnorm(N) -1), ncol=2)
X <- rbind(X1, X2)

y <- rep(c(1, -1), each=N)
## simplePlot(X, y)
lsvm <- showSVM(X, y, kernel="linear", C=10, use.bias=FALSE)
# gsvm <- showSVM(X, y, kernel="gaussian", C=10, width=1)
@

Let's move the margin.

<<label=easyMoveMargin>>=
X.harder <- rbind(X, matrix(c(0, 0), ncol=2))
y.harder <- c(y, 1)
lsvm <- showSVM(X.harder, y.harder, kernel='linear', C=10)
@

To extract the weight vector

\begin{equation}
  {\bf W} = \sum_{i=1}^N \alpha_i y_i {\bf x}_i
\label{eqn:wvector}
\end{equation}

@

<<label=svmCircle>>=
set.seed(123)
Xcircle <- circle(0, 0, 5, nv=100)
Xcircle <- cbind(Xcircle$x, Xcircle$y)
Xinside <- matrix(rnorm(200), ncol=2)

Xc <- rbind(Xcircle, Xinside)
yc <- rep(c(1, -1), each=100)

psvm <- showSVM(Xc, yc, kernel='poly', C=10, degree=2)
psvm <- showSVM(Xc, yc, kernel='poly', C=10, degree=2, wireframe=TRUE)

gsvm <- showSVM(Xc, yc, kernel='gaussian', C=1000, width=1)
gsvm <- showSVM(Xc, yc, kernel='gaussian', C=1000, width=1, wireframe=TRUE)
@

%% <<label=svmDecision>>=
%% ## Let's make 2D data from gaussians with different means
%% set.seed(123)
%% 
%% Xgaus <- matrix(c(rnorm(2 * N, 1, 1), rnorm(2 * N, -1, 1)), ncol=2)
%% simplePlot(Xgaus, y)
%% 
%% lsvm <- showSVM(Xgaus, y, kernel="linear", C=10)
%% lacc <- sum(predict(lsvm, Xgaus) == y) / length(y)
%% cat(sprintf("Linear svm accuracy: %.2f\n", lacc)
%% 
%% gsvm <- showSVM(Xgaus, y, kernel="gaussian", C=10, width=1)
%% gacc <- sum(predict(gsvm, X) == y) / length(y)
%% cat(sprintf("Gaussian svm accuracy: %.2f\n", gacc))
%% @

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Classification on Strings
\section{Classification on Strings}
\label{sec:classification_on_strings}

How do we project strings into a multi-dimensional space?

%% ============================================================================
%% Spectrum Kernel
\subsection{Spectrum Kernel}
\label{sub:spectrum_kernel}

Cite Christina

\begin{equation}
  k(x,x') = \sum_{u \in \Sigma^d} N(u, x) N(u, x')
\end{equation}

where $N(u,x)$ is the function that returns the number of occurrences of kmer
$u$ in the string $x$.

TODO: Classify promoters

<<label=promoters>>=
data(promoters, package="shikken")

@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Motivation
\section{Motivation}
\label{sec:motivation}
The RNA binding protein (RBP) known as \emph{NOVA-1} (and \emph{NOVA-2}) in higher eukaryotes, and pasilla (ps) in fly, has been previously implicated in playing a role in the regulation of alternative splicing.

<<label=novamm9distro>>=
data(annotated.genomes)
data(NOVA.mm9)
gd
nova.summary <- data.table(as.data.frame(nova.peaks))[, {
  list(score=sum(score))
}, by="exon.anno"]

gg.angle <- opts(axis.text.x=theme_text(angle=-45, hjust=0, vjust=1))

g <- ggplot(nova.summary, aes(x=exon.anno, y=score, fill=exon.anno)) +
  geom_bar(stat="identity") + theme_bw() + gg.angle +
  opts(title="NOVA binding site regions in mm9")
print(g)
@


%% ----------------------------------------------------------------------------
%% Spectrum Kernel
\subsubsection{Spectrum Kernel} % (fold)
\label{ssub:spectrum_kernel}



%% ----------------------------------------------------------------------------
%% Weighted Degree Kernel
\subsubsection{Weighted Degree Kernel}
\label{ssub:weighted_degree_kernel}

The \emph{weighted degree} (WD) kernel computes similarities between
sequences while taking positional information into account. The WD kernel
counts the exact co-occurrences of \emph{k}-mers at corresponding positions
of the two sequences being compared.

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=1]{figs/weighted-degree-kernel}
  \caption{
    Given two sequences $x_1$ and $x_2$ of equal length, the kernel
    computes a weighted sum of matching subsequences. Each matching
    subsequence makes a contribution $w_B$ depending on its length $B$,
    where longer matches contribute more significantly.}
  \label{fig:WDK}
\end{figure}

\begin{equation}
  k(x,x') = \sum_{k=1}^d \beta_k \sum_{i=1}^{l-k+1} \hbox{I}(u_{k,i}(x) = u_{k,i}(x'))
\end{equation}


%% ----------------------------------------------------------------------------
%% Weighted Degree Kernel with Shifts
\subsubsection{Weighted Degree Kernel with Shifts}
\label{ssub:weighted_degree_kernel_with_shifts}

The weighted degree kernel with shifts (the WDS kernel) shifts the two
sequences against each other in order to allow for small positional variations
of sequence motifs. It is conceptually a combination of the spectrum and WD
kernels.

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=1]{figs/weighted-degree-kernel-with-shifts}
  \caption{
    Given two sequences $x_1$ and $x_2$ of equal length, the WDS kernel produces
    a weighted sum to which each match in the sequences makes a contribution
    $\gamma_{k,p}$ depending on its length $k$ and relative position $p$, where
    longer matches at the same position contribute more significantly.
  }
  \label{fig:WDKS}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Learning preferred binding landscapes
\section{Learning preferred binding landscapes}
\label{sec:learning_preferred_binding_landscapes}


%% ============================================================================
%% Feature selection
\subsection{Feature selection}
\label{sub:feature_selection}


%% ============================================================================
%% Training
\subsection{Training}
\label{sub:training}



%% ============================================================================
%% Testing
\subsection{Testing}
\label{sub:testing}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Applying model on novel organism
\section{Applying model on novel organism}
\label{sec:applying_model_on_novel_organism}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Unsupervised learning with NFM
\section{Unsupervised learning with Nonnegative Matrix Factorization}
\label{sec:unsupervised_learning_with_nonnegative_matrix_factorization}
Non-negative matrix factorization


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Future things to explore
\section{Future things to explore}
\label{sec:future}
Novel Machine Learning Methods for MHC Class I Binding Prediction
http://www.fml.tuebingen.mpg.de/raetsch/members/raetsch/bibliography/WTAKR2010
http://www.fml.tuebingen.mpg.de/raetsch/lectures/talk-multitask-recomb2010.pdf


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements
\section{Acknowledgements}
\label{sec:acknowledgements}

I had to pick a few brains in order to get this lab together and I'd like to
thank them here.

\emph{Chris Widmer} in Gunnar Raetch's lab for provided instrumental help with
many of the nuts-and-bolts questions I had about the shogun toolbox. He also
provided  materials from some of his previous talks about shogun as well as
multi-task learning that I adapted for some of the material in this tutorial.

\emph{Raphael Pelossof} and \emph{Alvaro Gonzalez} for useful conversations.
They are both post-docs in my advisor's lab and are always happy to share their
expertise.

Finnaly, I'd like to thank my advisor \emph{Christina Leslie} for always being
happy to shine her flash-light on just about everything.

\bibliography{MLplay}
\bibliographystyle{plain}


\section{Session Information}
<<sessionInfo>>=
sessionInfo()
@

\end{document}
