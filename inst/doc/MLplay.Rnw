%\VignetteIndexEntry{Maching learning from NGS data}
%\VignetteKeywords{tutorial}
%\VignettePackage{MLplay}
\documentclass[11pt]{article}

\usepackage{colortbl}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{color}
\usepackage{xspace}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{subfigure}

\usepackage[boxed, linesnumbered]{algorithm2e}
\usepackage[
         colorlinks=true,
         linkcolor=blue,
         citecolor=blue,
         urlcolor=blue]{hyperref}

\usepackage{lscape}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Try to make things look purty
%% I like to use the fullpage!
\usepackage{fullpage}

%% Do not indent paragraphs
\usepackage[parfill]{parskip}

%% And fonts!
% For bold and small caps together
% http://stackoverflow.com/questions/699371/latex-small-caps-and-bold-face
\usepackage[T1]{fontenc}

%% Use Helvetica
\usepackage{times}
\usepackage[scaled=1]{helvet}     % PostScript font Helvetica for sans serif
\renewcommand{\rmdefault}{phv}    % Helvetica for roman type as well as sf
\renewcommand{\sfdefault}{phv}    % Helvetica for roman type as well as sf

% Inconsolata for monospaced fonts
% (fortunately this is distributed with the TeX-live distribution)
\usepackage{inconsolata}

%% End: Purty -----------------------------------------------------------------

\usepackage{Sweave}
\SweaveOpts{keep.source=TRUE,prefix.string=Rfigs/gen}

% define new colors for use
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darkred}{rgb}{0.6,0.0,0}
\definecolor{lightbrown}{rgb}{1,0.9,0.8}
\definecolor{brown}{rgb}{0.6,0.3,0.3}
\definecolor{darkblue}{rgb}{0,0,0.8}
\definecolor{darkmagenta}{rgb}{0.5,0,0.5}

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}
\newcommand{\Rcode}[1]{{\texttt{#1}}}
\newcommand{\software}[1]{\textsf{#1}}
\newcommand{\R}{\software{R}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}

%% Either
% \lhead{}
% \chead{Machine Learning and NGS data}
% \rhead{}
% \lfoot{}
% \cfoot{}
% \rfoot{\thepage\ of \pageref{LastPage}}
% \renewcommand{\headrulewidth}{1pt}
% \renewcommand{\footrulewidth}{1pt}

%% Or
\fancyhead[RO,RE]{\slshape \leftmark}
% Using fancy headers sometimes lets figures, etc. bleed into the header, this
% trick has fixed it for me in the past
\setlength\headsep{25pt}

\title{Learning from Sequencing Data with Support Vector Machines}

\author{
  Steve Lianoglou \\
  Memorial Sloan-Kettering Cancer Center \\
  \small{\texttt{lianos@cbio.mskcc.org}}
}

\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page

\maketitle

The goal of this tutorial is to provide a brief and intuitive introduction to
some machine learning techniques --- primarily support vector machines. Some
mathematical rigor will likely be sacrificed in order to appeal to intuition.
Technical concepts will be followed closely by practical examples in R to help
shed light on things that may at first seem to be too abstract. I will spend a
fair bit of time on the introduction because I think it is rather important to
understand. Often times, the ML approaches you apply will not work the first time
you try them. Getting things to work as you like will require (1) picking the
right data and carefully scrubbing it; and (2) understanding how your learning
algorithm functions, so you can better accomplish (1).

We will introduce string kernels and show how they can be used to learn from
sequencing data. An example of how one can begin to use them with HITS-CLIP
data to try to identify the preferred binding landscape for the
RNA binding protein \emph{NOVA-1/2} will be introduced.

This vignette is written in a style that is meant to be amenable to self study.
A live presentation of this material should probably be less verbose.

This document and package made its initial debut as a tutorial given at the
\emph{Advanced R / Bioconductor Workshop on High-Throughput Genetic Analysis}
hosted at the Fred Hutchinson Cancer Research Center in February, 2012. It will
continue to live (and receive updates) at the github repository listed below
in the hope that others may find it useful as a resource to learn from.

\begin{center}
  \url{https://github.com/lianos/BiocSeqSVM}
\end{center}

\clearpage

% \renewcommand{\baselinestretch}{.6}

\tableofcontents

% \thispagestyle{empty}

\vspace{.2in}

% \renewcommand{\baselinestretch}{1}

\clearpage

<<options,echo=FALSE>>=
options(width=75)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction
\section{Introduction}
\label{sec:introduction}

Machine learning (ML) methods play an increasingly important role in the fields
of biology and bioinformatics. Perhaps the largest deluge of the application of
ML techniques to biology happened after the widespread adoption of microarrays.
Microarrays make it easy for researchers to generate extremely high dimensional
datasets produced by measuring the expression of thousands of genes at a single
time point. Another floodgate of biological data has recently been opened as
the cost of next generation sequencing (NGS) has plummeted. There is no shortage
of ways scientists can now generate vast amount of data, but its interpretation
still remains a challenge. In this vein, the appropriate application of ML
can prove to be quite fruitful when trying to shed some light on the dark
corners of your data..

%% ============================================================================
%% Caveat emptor
\subsection{Caveat emptor}
\label{sub:caveat_emptor}

\begin{itemize}
  \item This tutorial is not meant to serve as a general purpose introduction to
  machine learning concepts. In particular, you will find very little time spent
  on discussing concepts such as cross validation, which is a \emph{crucial} step in
  assessing the performance and general utility of any machine learning algorithm.
  In this tutorial, more time is spent showing how support vector machines 
  (and different kernels) work, and how changing their parameters can affect the
  behavior of the algorithm. It can't be stressed enough that proper parameter
  tuning and model assessment (through cross validation, for instance) is
  \emph{absolutely essential}, even if it is only paid lip service here. In my
  opinion, once one gains a thorough understanding of the algorithms being used,
  applying something like cross validation should be quite trivial.
  
  For further ML references, especially in the context of \texttt{R} and bioconductor,
  the reader might be interested in the following resources:
  
  \begin{itemize}
    \item The \href{http://www.bioconductor.org/help/course-materials/2011/CSAMA/}{CSAMA 2011 workshop, machine learning primer}, by Vincent Carey.
    \item The \href{http://cran.r-project.org/web/packages/kernlab/}{vignette from the caret package}, by Max Kuhn.
  \end{itemize}
  
  \item We will be exploring support vector machines through a new R library
  I am authoring called \href{https://gihub.com/lianos/shikken}{shikken}.
  Shikken is a wrapper to the
  \href{http://www.shogun-toolbox.org/}{Shogun machine learning toolbox}.
  a \texttt{C++} library focussed on large scale kernel methods. Shogun
  has the largest variety of SVM solvers and kernel methods I have encountered
  in one package. It also has a focus on kernel methods that are useful in
  biology --- particularly ones that work on sequence features. My hope is that
  being able to access Shogun's functionality through shikken will be helpful
  since it provides an interface to Shogun that an \texttt{R} user will likely be
  more comfortable with. See Section~\ref{sec:shogun} for more information about how
  to use shikken effectively (ie. what bugs you should be looking out for!).
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Support Vector Machines
\section{Support vector machines and large margin classifiers}
\label{sec:support_vector_machines}

[Note: the details and examples used to illustrate the fundamentals of how SVMs
work are largely taken and summarized from~\cite{BenHur:2008ec,BenHur:2009ch}.
Yet another introduction to SVMs in the context of computational biology can
be found here~\cite{Noble:2006br}]

The Support Vector Machine (SVM) is a versatile and state-of-the-art classifier
that was first introduced by Boser, Guyon and Vapnik~\cite{Boser:1992uo}.
As shown in Figure~\ref{fig:svmdecision}, an advantageous property of the SVM,
is that it creates a large margin linear classifier that, typically, lives in high
dimensional spaces. The margin of a classifier is simply defined as the shortest
distance from the decision boundary to the first training example (that is not
actively being ignored!). Large margin classifiers have the tendency to
generalize well to unseen data --- intuitively because they provide more
``breathing room'' between the decision boundary and the next unseen data point
you want to classify. The SVM finds the optimal decision boundary by minimizing
$\| {\bf w} \|^2$ in the convex optimization problem shown in
Equation~\ref{eqn:primal}, and depicted in Figure~\ref{fig:svmdecision}.

\begin{align}
  \min_{ {\bf w}, b, \xi} \mbox{ } \frac {1} {2} \| {\bf w} \|^2 + C \sum_{i=1}^n \xi_i \nonumber \\
  \mbox{s.t : } y_i \left( \left\langle {\bf w}, {\bf x}_i \right\rangle + b \right) \ge 1 - \xi_i, \nonumber \\
  \xi_i \ge 0, \mbox{for } i = 1 \dots n
\label{eqn:primal}
\end{align}


\begin{figure}[htbp]
  \centering
    \includegraphics[scale=0.6]{figs/decision-boundaries.pdf}
  \caption{
    The anatomy of a decision boundary for a large (and soft) margin classifier.
    \textbf{A} The red, green, and blue decision boundaries on the left all separate
    the two-class training data (grey and white points) equally well.
    \textbf{B} The decision boundary found by the SVM is on the right.
    The exact orientation of the boundary is specified by its orthogonal
    vector ${\bf w}$. The boundary is said to have a \emph{large margin} because
    the smallest distance to any training point is maximized. The example training
    point outlined in red is ignored when defining the margin (the dashed lines), but the
    machine pays a penalty that is proportional to the magnitude of the margin
    violation $\xi_i$. The examples with dark borders are the \emph{support vectors},
    which will ultimately orient the direction of $\vec{W}$. The size of the margin for
    this classifier is identified by the blue line. This figure was adapted from the
    Wikipedia's \href{http://en.wikipedia.org/wiki/Support_vector_machine}{Support vector machines} page on 2012-02-22.
  }
  \label{fig:svmdecision}
\end{figure}

Minimizing $\| {\bf w} \|^2$ in Equation~\ref{eqn:primal} is equivalent
to maximizing the margin. The inequality constraints on the second line
ensure that the examples are correctly classified
($y_i \left( \left\langle {\bf w}, {\bf x}_i \right\rangle + b \right) \ge 1$).
It is often the case, however, that data is not cleanly separable and the
\emph{slack variables} $\xi_i$ are introduced to accommodate these situations.
The slack variables permit examples to lie within the margin or to be
misclassified while still finding a large margin over the rest of the training
data. To discourage excess use of slack variables, a cost parameter
$C$ is also introduced. When training an SVM, it is important to identify the
best value for $C$ given the data you have available to you. We will briefly
explain how to do this later in Section~\ref{sec:model_refinement}.

\paragraph{Basic SVMs in R}
\label{par:basic_svms_in_r}

Let's fire up \texttt{R} and create a dataset that almost looks like that data in
Figure~\ref{fig:svmdecision} so we can see how to classify it using the
SVM methods available in shikken
\footnote{
  You can also use SVMs in \textbf{R} with the \texttt{e1071} and \texttt{kernlab}
  packages. \texttt{kernlab} also has an implementation of the spectrum kernel
  you can use.
}.
 
<<initialize, results=hide, echo=TRUE, eval=TRUE>>=
library(BiocSeqSVM)
library(shikken)

## Create two class data
set.seed(123)
N <- 50
X1 <- matrix(c(rep(-2, N) - rnorm(N, 1), rnorm(N) + 2), ncol=2)
X2 <- matrix(c(rep( 2, N) + rnorm(N, 1), rnorm(N) - 2), ncol=2)

## Note that it's generally a good idea to scale your data
## (especially if its continuous) so we do that here.
X <- scale(rbind(X1, X2))
y <- rep(c(1, -1), each=N)
@

To use an SVM to clasify a dataset, shikken provides the \texttt{SVM}
method which accepts a dataset \texttt{x} of observations (rows), and
a label vector \texttt{y}. The \texttt{C} parameter is the same $C$
described in Formula~\ref{eqn:primal}, ie. the cost for non-zero
slack variables.

<<label=easyMargin,fig=TRUE,include=FALSE>>=
lsvm <- SVM(X, y, C=100)
plotDecisionSurface(lsvm, X, y)

## Does it accurately classify the data?
table(predict(lsvm, X), y)
@

The \texttt{plotDecisionSurface} function draws the data points
over a contour plot of the decision surface from the SVM
(Figure~\ref{fig:easymargin}). The decision
boundary is located at the $z=0$ contour. The points in red are the
examples that serve as the support vectors --- note how these examples
orient the decision boundary. As examples get further and further away
from the decision boundary, their decision values get increasingly higher.
When the SVM produces a high (absolute) decision value, you can think of
it as giving a high-confidence prediction of the class label. Positive
decision values belong to one class, negative values belong to the other.

\paragraph{Question}
\emph{What happens to the decision boundary and support vectors
when we decrease the value of $C$ to either $0.5$ or $0.1$.
Why does that happen?}

<<label=easyPlay,fig=FALSE>>=
lsvm <- SVM(X, y, C=0.5)
plotDecisionSurface(lsvm, X, y)

lsvm <- SVM(X, y, C=0.1)
plotDecisionSurface(lsvm, X, y)
@

\paragraph{Add some noise}

If you have ever worked with real data, it will come to no surprise to you that
most datasets are not as well behaved as the toy example we've
tried so far. Let's imagine there was some noise in a measurement that
produced an outlier of our negatively labeled data that looks a lot
closer to our negative data than the positive data.

<<label=easyOutlier,fig=FALSE>>=
X.out <- rbind(X, t(c(-1, -0.5)))
y.out <- c(y, -1)

lsvm <- SVM(X.out, y.out, C=100)
plotDecisionSurface(lsvm, X.out, y.out)

table(predict(lsvm, X.out), y.out)
@

The SVM is doing its job and classifying the data perfectly, but is this
desirable?

\paragraph{Questions}
\emph{What is wrong, if anything, with the this decision boundary?
How can we fix it?
Is the best classifier you can build the one that always classifies your
training data perfectly?}

<<label=easySoftMargin,echo=FALSE,include=FALSE,fig=TRUE>>=
lsvm <- SVM(X.out, y.out, C=1)
plotDecisionSurface(lsvm, X.out, y.out)
@

\begin{figure}[htbp]
  \centering  
  \mbox{\subfigure{\includegraphics[width=3in]{Rfigs/gen-easyMargin.pdf}}\quad
  \subfigure{\includegraphics[width=3in]{Rfigs/gen-easySoftMargin.pdf} }}
  \caption{
    Finding large margin hyperplane in well behaved data. A toy dataset is
    shown that is intuitively easy to classify. On the left, there isn't
    much left up to interpretation as to whether or not the SVM can finds
    the best separating hyperplane that has the largest margin. In the
    example on the right, imagine we observed an outlier in our set of
    positively labeled data. Trying to fit the hyperplane to classify that
    example will severely skew the decision boundary and rotate it to the
    right, as we saw in the example code when $C=100$. Setting $C = 0.5$
    allows us to recover a classifier with a larger margin by allowing the
    SVM ``enough slack'' to misclassify the ``outlier'' (shown on right).
  }
  \label{fig:easymargin}
\end{figure}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Kernels
\section{Kernels (and the dual of the SVM)}
\label{sec:kernels}

As we saw in the previous example, not all data is well behaved --- it is not
always easy to find a linear separator that can split your two classes
of data. Still, the challenge of dealing with outliers was one that the SVM
seemed to handle quite easily. But what happens if your data is really
impossible to split, as shown in the code below and in Figure~\ref{fig:circledata}?

<<label=circleData,fig=TRUE,include=FALSE>>=
## The guts of makeCircle taken from plotrix package
Xcircle <- makeCircle(0, 0, 5, nv=100)
Xcircle <- cbind(Xcircle$x, Xcircle$y)
Xinside <- matrix(rnorm(200), ncol=2)

Xc <- rbind(Xcircle, Xinside)
yc <- rep(c(1, -1), each=100)

simplePlot(Xc, yc)
@

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=.1]{Rfigs/gen-circleData}
  \caption{How can you separate this with a linear classifier?}
  \label{fig:circledata}
\end{figure}

A reasonable way to split the data in Figure~\ref{fig:circledata}
with a linear classifier is to first project it into a higher
dimensional space, then find the hyperplane that can separate
the data in this new space. A ``simple''\footnote{
  ``Simple'' is in quotes here, because this
  is ``simple'' to think about, but if you are working with thousands
  (or millions) of data points, and you want to project them into
  a $50,000$ dimensional space, the ``simple'' task of just holding
  all of this data in memory isn't so simple anymore.
} way to do this is to
explicitly project every data point into the new space and then
find the hyperplane like ``you normally would.'' Your ${\bf w}$
vector would then  also be living in this higher dimensional space
and your new discriminant function would then look like what is shown in
Equation~\ref{eqn:phidisc}. 

\begin{align}
  f(x) = \left\langle {\bf w}, \phi ({\bf x}) \right\rangle + b
\label{eqn:phidisc}
\end{align}

There is a smarter way to go, and it involves using kernels and
``the kernel trick''. You can think of a kernel function
$k({\bf x}, {\bf x'}) \ge 0$ as one that takes two examples
and returns a real-valued number that tells you how similar the two
data points are (higher is more similar).

Now we have to travel into the weeds a bit ...

There is a \emph{dual} formulation of the SVM objective function that
uses Lagrange multipliers to make the optimization problem of the \emph{primal}
(Equation~\ref{eqn:prmial}) easier to solve (apparently!).
Its optimal value is the same as the primal one under certain
constraints\cite{BenHur:2008ec}. To help keep
our sanity, the derivation of the dual from the primal is skipped here,
but can be found in other SVM references~\footnote{If you're interested
in the punishment, Andrew Ng has his course materials for the ML class
he teaches at Stanford online. His notes on SVMs actually have a nice
description of how this is done.} The dual representation is given in
Equation~\ref{eqn:dual}.

\begin{align}
  \max_a \sum_{i=1}^n \alpha_i - \frac {1} {2} \sum_{i=1}^n \sum_{j=1}^n y_i y_j \alpha_i \alpha_j \left\langle {\bf x}_i, {\bf x}_j \right\rangle \nonumber  \\
  \mbox{s.t : } \sum_{i=1}^n y_i \alpha_i = 0; \mbox{ and } 0 \leq a_o \leq C
\label{eqn:dual}
\end{align}

It can also be shown that the weight vector ${\bf w}$ can be
expressed solely as a function over the examples ${\bf x}_i$ and their optimal values of $\alpha_i$ (found in Equation~\ref{eqn:dual}), as shown in Equation~\ref{eqn:wvector}.

\begin{align}
  {\bf w} = \sum_{i=1}^n y_i \alpha_i {\bf x}_i
\label{eqn:wvector}
\end{align}

Using the kernel trick we can rewrite our discriminant function from
Equation~\ref{eqn:wvector} to:

\begin{align}
  f({\bf x}) = \sum_{i=1}^n y_i \alpha_i k({\bf x}_i, {\bf x}) + b
\label{eqn:wkernel}
\end{align}

The SVM will try to set many of the $\alpha_i$ values to 0, which will give
us a sparse \emph{representation} of ${\bf w}$ using only the examples where
$\alpha_i > 0$ --- these examples are called the \emph{support vectors} and lie on
(or within) the margin. The support vectors are the red examples shown in the \texttt{plotDecisionSurface}) function.

\paragraph{Important take away from the dual and kernels}
\begin{itemize}
  \item We can use kernels to calculate similarities between two objects
        by implicitly mapping them to different feature spaces.
  \item The SVM can be solved using this implicit mapping (Equation~\ref{eqn:dual}), 
        which means you can work in, say, a $50,000$ dimensional space without having
        to explicitly generate feature vectors of $50,000$ dimensions for all of your
        data points
  \item The decision boundary of the SVM has a sparse representation which only
        relies on the $\alpha_i$ values from your support vectors, and the support
        vectors themselves, which you keep in their ``native'' (lower dimensional)
        feature space.
  \item Kernel functions are flexible and can be developed to take advantage
        of relationships in your data. In this tutorial, we will show kernels
        for numeric data which are generally useful, but also kernels which define
        similarities between different properties of strings.
\end{itemize}

\clearpage

%% ============================================================================
%% Numeric Kernels
\subsection{Numeric Kernels}
\label{sub:numeric_kernels}

\paragraph{Polynomial kernel}
\label{par:polynomial_kernel}

\begin{align}
  k_{d,k}^{\mbox{polynomial }} ({\bf x}, {\bf x'}) = \left( \left\langle {\bf x}, {\bf x'} \right\rangle + k \right)^d
\label{eqn:polykernel}
\end{align}

<<label=svmPoly,fig=TRUE,include=FALSE>>=
psvm <- SVM(Xc, yc, kernel="poly", C=10, degree=2)
table(predict(psvm, Xc), yc)

## Set wireframe=TRUE to see the decision surface in 3D
plotDecisionSurface(psvm, Xc, yc, wireframe=FALSE)
@

<<label=svmPoly3D,fig=TRUE,include=FALSE,echo=FALSE>>=
plotDecisionSurface(psvm, Xc, yc, wireframe=TRUE)
@

\begin{figure}[htbp]
  \centering  
  \mbox{\subfigure{\includegraphics[width=3in]{Rfigs/gen-svmPoly.pdf}}\quad
  \subfigure{\includegraphics[width=3in]{Rfigs/gen-svmPoly3D.pdf} }}
  \caption{
    Using a 2nd degree polynomial kernel to find a large margin separating
    hyperplane in three dimensions. The decision surface is shown in
    both two (left) and three (right) dimensions, here.
  }
  \label{fig:polykernel}
\end{figure}

\emph{Look at what the decision surface does when you increase the value for
\texttt{C} and set it to 10000. Can you intuitively describe why
that happens?}

\paragraph{Gaussian Kernel}
\label{par:gaussian_kernel}
Another useful kernel on numerical data is the gaussian kernel:

\begin{align}
  k_{d,k}^{\mbox{gaussian }} ({\bf x}, {\bf x'}) = \exp \left( - \frac {1} {\sigma} \| {\bf x} - {\bf x'} \|^2 \right)
\end{align}

<<label=svmGaus,fig=TRUE,include=FALSE,echo=TRUE>>=
gsvm <- SVM(Xc, yc, kernel="gaussian", C=1000, width=1)
table(predict(psvm, Xc), yc)
plotDecisionSurface(gsvm, Xc, yc, wireframe=FALSE)
@

<<label=svmGaus3D,fig=TRUE,include=FALSE,echo=FALSE>>=
plotDecisionSurface(gsvm, Xc, yc, wireframe=TRUE)
@

\begin{figure}[htbp]
  \centering  
  \mbox{\subfigure{\includegraphics[width=3in]{Rfigs/gen-svmGaus.pdf}}\quad
  \subfigure{\includegraphics[width=3in]{Rfigs/gen-svmGaus3D.pdf} }}
  \caption{
    Using a gaussian kernel to split non-separable data. The decision surface
    for the SVM is shown in both two- and three-dimensional space
  }
  \label{fig:gausskernel}
\end{figure}



\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Classification on Strings
\section{Classification on strings using the spectrum kernel}
\label{sec:spectrum}

How do we project strings into a multi-dimensional space?

%% ============================================================================
%% Spectrum Kernel
\subsection{Spectrum Kernel}
\label{sub:spectrum_kernel}

The spectrum kernel.~\cite{Leslie:2002tx}
Mismatch string kerenls~\cite{Leslie:2004kt}

\begin{align}
  k(x,x') = \sum_{u \in \Sigma^d} N(u, x) N(u, x')
\end{align}

where $N(u,x)$ is the function that returns the number of occurrences of kmer
$u$ in the string $x$ and $\Sigma^d$ is the set of all words of length $d$. An intuitive representation of the kernel kernel is shown in Figure~\ref{fig:spectrum}

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=1]{figs/spectrum-kernel}
  \caption{
    Calculating the spectrum kernel, with and without mismatches.
    A naive way to calculate the spectrum kernel of degree 2 is to
    create a vector of length $4^2 = 16$ which is used to count the number
    of occurrences of each $k$-mer in the string.
    A sliding window (red box) equal to length as the degree of the kernel
    slides along the string.
    The counts of exact $k$-mers are incremented as they appear in the window
    (above). Mismatches can be accommodated by
    incrementing the count of all of the $k$-mers in the $n$-mismatch
    neighborhood around the current $k$-mer in the window (below, $n=1$).
    The final kernel evaluation between two strings is simply the
    dot product of their vector representations.
    The red arrow above the box points in the direction the window is sliding.
    The black arrows point to the bins that have just been incremented
    by observing the $k$-mer in the window.
  }
  \label{fig:spectrum}
\end{figure}

To be a bit more clea\texttt{R}, using \texttt{Biostrings} we can explicitly compute
the feature space for a set of \texttt{XStringSet} objects $X$ used by the spectrum
kerenl of degree $d$ via a call to \texttt{ol} , a call to
\texttt{oligonucleotideFrequency(X, d)}.

As an example of how to use the spectrum kernel, we'll use the ``promoter gene
sequences'' dataset from the UCI machine learning repository. This dataset has a
set of 50 promoter sequences from E. coli and another set of 50 non-promoter sequences.
The data is provided as a \texttt{DNAStringSet} object in the \texttt{promoters} data
object from the shikken library. The class of each promoter is stored in the
\texttt{\$class} column of the \texttt{DataFrame} attached to the \texttt{DNAStringSet}.

<<label=promoters>>=
data(promoters, package="shikken")
head(promoters)
y <- values(promoters)$class

m <- SVM(promoters, y, kernel='spectrum', degree=4, threads=2)
table(predict(m, promoters), y)
@

\emph{Exercise: How long do the kmers have to be to build a strong classifier?}


It is clear that we can classify the promoters correctly using a spectrum
kernel of degree four. While building an accurate classifier is sometimes
sufficient for a given task, it can also be important to understand what
it means --- how is the SVM able to identify promoters vs. non promoters?
It would be nice if we could gain some insight into what the SVM is doing.
There might be some interesting biology there.

You can also create the ``spectrum features'' using
\texttt{oligonucleotideFrequency} and use something like glmnet\footnote{
  If you are trying to do a penalized ordinary/logistic regression,
  I have a buckshot package at http://gihub.com/lianos/buckshot that
  wraps the large scale parallelized solver called ``shotgun'' written
  by the SELECT lab at CMU.
}

\paragraph{Practical time}

\begin{itemize}
  \item Visualize kmers using ${\bf w}$ vector
  \item POIMs~\cite{Ratsch:2006il} are a principled way to visualize results
  \item Try similar task in mouse. See \texttt{demos/predict-mm9-tss.R}
  \item Do HITS-CLIP example

  \item mention multi-task learning
\end{itemize}

\paragraph{Multitask learning}

A general version of the loss we are optimizing

\begin{align}
  \min_{\mathbf{w}} \underbrace{L(\mathbf{w}|X,Y)}_{\textnormal{loss-term}} + \underbrace{R(\mathbf{w})}_{\textnormal{regularizer}}.
\end{align}

We could try to improve the mouse/fly joint model via multi task learning

\begin{align}
  \min_{\mathbf{w}_1, ..., \mathbf{w}_M} \sum_{i=1}^M \underbrace{L(\mathbf{w}_i|X,Y)}_{\textnormal{loss-term}} + \sum_{i=1}^M \underbrace{R(\mathbf{w}_i)}_{\textnormal{regularizer}} + \underbrace{R_{MTL}(\mathbf{w}_1, ..., \mathbf{w}_M)}_{\textnormal{MTL-regularizer}}.
\end{align}


%% ============================================================================
%% Feature Selection
\subsection{Feature Selection}
\label{sub:feature_selection}

TODO: Extract ${\bf W}$ and visualize with \texttt{wordcloud} and recursive
feature elimination (RFE).

Learning interpretable SVMs for biological sequence classification~\cite{Ratsch:2006il}.
positional oligomer importance matrices (POIMs)~\cite{Sonnenburg:2008do}.


The elastic net can do similar~\cite{Zou:2005elastic}.
TODO: Show same example using \texttt{glmnet}.

%% ============================================================================
%% Cross validation
\subsection{Cross validation}
\label{sub:cross_validation}

Use \texttt{caret,createFolds function}. TODO: Cross Validation

%% ============================================================================
%% Characterize NOVA binding sites
\subsection{Characterize NOVA binding sites}
\label{sub:characterize_nova_binding_sites}

The RNA binding protein (RBP) known as \emph{NOVA-1} (and \emph{NOVA-2}) in higher eukaryotes, and pasilla (ps) in fly, has been previously implicated in playing a role in the regulation of alternative splicing.

Where does NOVA like to bind the pre-mRNA?

<<label=novamm9distro,fig=TRUE,include=TRUE>>=
data(annotated.genomes)
data(NOVA.mm9)

head(nova.peaks)
dt <- data.table(as.data.frame(nova.peaks))
nova.summary <- dt[, list(score=sum(score)), by='exon.anno']

gg.angle <- opts(axis.text.x=theme_text(angle=-45, hjust=0, vjust=1))

g <- ggplot(nova.summary, aes(x=exon.anno, y=score, fill=exon.anno)) +
  geom_bar(stat="identity") + theme_bw() + gg.angle +
  opts(title="NOVA binding site regions in mm9")
print(g)
@

Try to use a spectrum kernel to identify preferred binding landscape of
\emph{NOVA-1/2} RNA binding protein.


%% ============================================================================
%% Training
\subsection{Training}
\label{sub:training}

training splits

%% ============================================================================
%% Testing
\subsection{Testing}
\label{sub:testing}
testing splits

%% ============================================================================
%% Apply NOVA binding model to Drosophila homolog
\subsection{Apply NOVA binding model to Drosophila homolog}
\label{sub:apply_nova_binding_model_to_drosophila_homolog}

Hit ps data

%% ============================================================================
%% Using multitask learning to combine datasets
\subsection{Using multitask learning to combine datasets}
\label{sub:using_multitask_learning_to_combine_datasets}

multitask learning

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Higher resolution sequence kernels
\section{Higher resolution sequence kernels}
\label{sec:higher_resolution_sequence_kernels}

The spectrum kernel has no concept of position --- a kmer gets the same weight
if it appears at the beginning of a string or at the end.

%% ============================================================================
%% Weighted Degree Kernel
\subsection{Weighted Degree Kernel}
\label{sub:weighted_degree_kernel}

The \emph{weighted degree} (WD) kernel computes similarities between
sequences while taking positional information into account. The WD kernel
counts the exact co-occurrences of \emph{k}-mers at corresponding positions
of the two sequences being compared.

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=1]{figs/weighted-degree-kernel}
  \caption{
    Given two sequences $x_1$ and $x_2$ of equal length, the kernel
    computes a weighted sum of matching subsequences. Each matching
    subsequence makes a contribution $w_B$ depending on its length $B$,
    where longer matches contribute more significantly.}
  \label{fig:WDK}
\end{figure}

\begin{align}
  k(x,x') = \sum_{k=1}^d \beta_k \sum_{i=1}^{l-k+1} \hbox{I}(u_{k,i}(x) = u_{k,i}(x'))
\end{align}


%% ============================================================================
%% Weighted Degree Kernel with Shifts
\subsection{Weighted Degree Kernel with Shifts}
\label{sub:weighted_degree_kernel_with_shifts}

The weighted degree kernel with shifts (the WDS kernel) shifts the two
sequences against each other in order to allow for small positional variations
of sequence motifs. It is conceptually a combination of the spectrum and WD
kernels.

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=1]{figs/weighted-degree-kernel-with-shifts}
  \caption{
    Given two sequences $x_1$ and $x_2$ of equal length, the WDS kernel produces
    a weighted sum to which each match in the sequences makes a contribution
    $\gamma_{k,p}$ depending on its length $k$ and relative position $p$, where
    longer matches at the same position contribute more significantly.
  }
  \label{fig:WDKS}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Future things to explore
\section{Future things to explore}
\label{sec:future}

\begin{itemize}
  \item Multiple kernel learning
  \item Multitask learning
\end{itemize}

% Novel Machine Learning Methods for MHC Class I Binding Prediction
% http://www.fml.tuebingen.mpg.de/raetsch/members/raetsch/bibliography/WTAKR2010
% http://www.fml.tuebingen.mpg.de/raetsch/lectures/talk-multitask-recomb2010.pdf


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements
\section{Acknowledgements}
\label{sec:acknowledgements}

I had to pick a few brains in order to get this lab together and I'd like to
thank them here.

\emph{Chris Widmer} in Gunnar Raetch's lab for provided instrumental help with
many of the nuts-and-bolts questions I had about the shogun toolbox. He also
provided  materials from some of his previous talks about shogun as well as
multi-task learning that I adapted for some of the material in this tutorial.

\emph{Raphael Pelossof} and \emph{Alvaro Gonzalez} for useful conversations.
They are both post-docs in my advisor's lab and are always happy to share their
expertise.

Finnaly, I'd like to thank my advisor \emph{Christina Leslie} for always being
happy to shine her flash-light on just about everything.

\bibliography{MLplay}
\bibliographystyle{plain}


\section{Session Information}
<<sessionInfo>>=
sessionInfo()
@

\end{document}
