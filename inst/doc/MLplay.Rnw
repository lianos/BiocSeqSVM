%\VignetteIndexEntry{Maching learning from NGS data}
%\VignetteKeywords{tutorial}
%\VignettePackage{MLplay}
\documentclass[11pt]{article}

\usepackage{colortbl}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{color}
\usepackage{xspace}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{subfigure}

\usepackage[boxed, linesnumbered]{algorithm2e}
\usepackage[
         colorlinks=true,
         linkcolor=blue,
         citecolor=blue,
         urlcolor=blue]{hyperref}

\usepackage{lscape}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Try to make things look purty
%% I like to use the fullpage!
\usepackage{fullpage}

%% Do not indent paragraphs
\usepackage[parfill]{parskip}

%% And fonts!
% For bold and small caps together
% http://stackoverflow.com/questions/699371/latex-small-caps-and-bold-face
\usepackage[T1]{fontenc}

%% Use Helvetica
\usepackage{times}
\usepackage[scaled=1]{helvet}     % PostScript font Helvetica for sans serif
\renewcommand{\rmdefault}{phv}    % Helvetica for roman type as well as sf
\renewcommand{\sfdefault}{phv}    % Helvetica for roman type as well as sf

% Inconsolata for monospaced fonts
% (fortunately this is distributed with the TeX-live distribution)
\usepackage{inconsolata}

%% End: Purty -----------------------------------------------------------------

\usepackage{Sweave}
\SweaveOpts{keep.source=TRUE,prefix.string=Rfigs/gen}

% define new colors for use
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darkred}{rgb}{0.6,0.0,0}
\definecolor{lightbrown}{rgb}{1,0.9,0.8}
\definecolor{brown}{rgb}{0.6,0.3,0.3}
\definecolor{darkblue}{rgb}{0,0,0.8}
\definecolor{darkmagenta}{rgb}{0.5,0,0.5}

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}
\newcommand{\Rcode}[1]{{\texttt{#1}}}
\newcommand{\software}[1]{\textsf{#1}}
\newcommand{\R}{\software{R}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}

%% Either
% \lhead{}
% \chead{Machine Learning and NGS data}
% \rhead{}
% \lfoot{}
% \cfoot{}
% \rfoot{\thepage\ of \pageref{LastPage}}
% \renewcommand{\headrulewidth}{1pt}
% \renewcommand{\footrulewidth}{1pt}

%% Or
\fancyhead[RO,RE]{\slshape \leftmark}
% Using fancy headers sometimes lets figures, etc. bleed into the header, this
% trick has fixed it for me in the past
\setlength\headsep{25pt}

\title{Learning from Sequencing Data with Support Vector Machines}

\author{
  Steve Lianoglou \\
  Memorial Sloan-Kettering Cancer Center \\
  \small{\texttt{lianos@cbio.mskcc.org}}
}

\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page

\maketitle

The goal of this tutorial is to provide a brief and intuitive introduction to
some machine learning techniques --- primarily support vector machines. Some
mathematical rigor will likely be sacrificed in order to appeal to intuition.
Technical concepts will be followed closely by practical examples in R to help
shed light on thing that may seem to be, at first, too abstract.

We will introduce string kernels and show how they can be used to learn from
sequencing data. An example of how you can begin to use them with HITS-CLIP
data to try to identify the preferred binding landscape for the
RNA binding protein \emph{NOVA-1/2} will be introduced.

This vignette is written in a style that is meant to be amenable to self study.
A live presentation of this material should probably be less verbose.

This document and package made its initial debut as a tutorial given at the
\emph{Advanced R / Bioconductor Workshop on High-Throughput Genetic Analysis}
hosted at the Fred Hutchinson Cancer Research Center in February, 2012. It will
continue to live (and receive updates) in the hope that others may find it
useful to learn from at the following github repository:

\begin{center}
  \url{https://github.com/lianos/BiocSeqSVM}
\end{center}

\clearpage

\renewcommand{\baselinestretch}{.6}

\tableofcontents

\thispagestyle{empty}

\vspace{.2in}

\renewcommand{\baselinestretch}{1}

\clearpage

<<options,echo=FALSE>>=
options(width=75)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction
\section{Introduction}
\label{sec:introduction}

Machine learning (ML) methods play an increasingly important role in the fields
of biology and bioinformatics. Perhaps the largest deluge of the application of
ML techniques to biology happened after the widespread adoption of microarrays.
Microarrays make it easy for researchers to generate extremely high dimensional
datasets produced by measuring the expression of thousands of genes at a single
time point --- their interpretation, however, is often a challenge, and this is
where the smart application of ML techniques can really pay off.

%% ============================================================================
%% Caveat emptor
\subsection{Caveat emptor}
\label{sub:caveat_emptor}

\begin{itemize}
  \item This tutorial is not meant to serve as a general purpose introduction to
  machine learning concepts. Other tutorials are available that you can find online
  that can serve this purpose. In the context of machine learning and R, these
  will likely be helpful:
  
  \begin{itemize}
    \item The \href{http://www.bioconductor.org/help/course-materials/2011/CSAMA/}{CSAMA 2011 workshop, machine learning primer}, by Vincent Carey.
    \item The \href{http://cran.r-project.org/web/packages/kernlab/}{vignette from the caret package}, by Max Kuhn.
  \end{itemize}
  
  \item We will be exploring support vector machines through a new R library
  I am authoring called \href{https://gihub.com/lianos/shikken}{shikken}.
  Shikken is a wrapper to the
  \href{http://www.shogun-toolbox.org/}{Shogun machine learning toolbox}.
  a \texttt{C++} library focussed on large scale kernel methods. Shogun
  has the largest variety of SVM solvers and kernel methods I have encountered
  in one package. It also has a focus on kernel methods that are useful in
  biology --- particularly ones that work on sequence features. My hope is that
  being able to access Shogun's functionality through shikken will be helpful
  since it provides an interface to Shogun that an R user will likely be more
  comfortable with. See Section~\ref{sec:shogun} for more information about how
  to use shikken effectively (ie. what bugs you should be looking out for!).
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Support Vector Machines
\section{Support vector machines and large margin classifiers}
\label{sec:support_vector_machines}

[Note: the details and examples to illustrate the fundamentals of how SVMs
work are largely taken and summarized from~\cite{BenHur:2008ec,BenHur:2009ch}.
William Noble has also authored another introduction to SVMs that appeals to
intuition over mathematical rigor~\cite{Noble:2006br}]

The Support Vector Machine (SVM) is a versatile and state-of-the-art classifier
that was first introduced by Boser, Guyon and Vapnik~\cite{Boser:1992uo}.
As shown in Figure~\ref{fig:svmdecision}, an advantageous property of the SVM,
is that it creates a large margin linear classifier that, typically, lives in high
dimensional spaces. The margin of a classifier is simply defined as the shortest
distance from the decision boundary to the first training example (that is not
actively being ignored!). Large margin classifiers have the tendency to
generalize well to unseen data --- intuitively because they provide more
``breathing room'' between the decision boundary and the next unseen data point
you want to classify.

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=0.5]{figs/decision-boundaries.pdf}
  \caption{
    The anatomy of a decision boundary for a large (and soft) margin classifier.
    \textbf{A} The red, green, and blue decision boundaries all separate the 
    two-class training data (tgrey and white points) equally well.
    equally well. The decision boundary found by the SVM has a \emph{large margin}
    because the smallest distance to any training point is maximized. The example
    outlined in red is ignored when defining the margin (the dashed lines), but the
    machine pays a penalty that is proportional to the magnitude of the margin
    violation $\xi_i$. The examples with dark borders are the support vectors, which
    orient the direction of $\vec{W}$. The size of the margin for this classifier is
    identified by the blue line.\footnote{
      This figure was adapted from wikipedia (2012-02-22):
      \url{http://en.wikipedia.org/wiki/Support_vector_machine}
    }
  }
  \label{fig:svmdecision}
\end{figure}

To find the large margin decision boundary, the SVM solves the following
convex optimization problem:

% Hard margin primal
% \begin{align}
%   \min_{ {\bf w}, b} \mbox{ } \frac {1} {2} \| {\bf w} \|^2 \nonumber \\
%   \mbox{s.t : } y_i \left( \left\langle {\bf w}, {\bf x}_i \right\rangle + b \right) \ge 1 \nonumber \\
%   \mbox{for } i = 1 \dots n
% \label{eqn:primal}
% \end{align}

\begin{align}
  \min_{ {\bf w}, b, \xi} \mbox{ } \frac {1} {2} \| {\bf w} \|^2 + C \sum_{i=1}^n \xi_i \nonumber \\
  \mbox{s.t : } y_i \left( \left\langle {\bf w}, {\bf x}_i \right\rangle + b \right) \ge 1 - \xi_i, \nonumber \\
  \xi_i \ge 0, \mbox{for } i = 1 \dots n
\label{eqn:softmargin}
\end{align}

Minimizing $\| {\bf w} \|^2$ in Equation~\ref{eqn:softmargin} is equivalent
to maximizing the margin. The inequality constraints on the second line
ensure that the examples are correctly classified
($y_i \left( \left\langle {\bf w}, {\bf x}_i \right\rangle + b \right) \ge 1$).
It is often the case, however, that data is not cleanly separable and the
\emph{slack variables} $\xi_i$ are introduced to accommodate these situations.
The slack variables permit examples to lie within the margin or to be
misclassified while still finding a large margin over the rest of the training
data. To discourage excess use of slack variables, a cost parameter
$C$ is also introduced. When training an SVM, it is important to identify the
best value for $C$ given the data you have available to you. We will briefly
explain how to do this later in Section~\ref{sec:model_refinement}.

\paragraph{Basic SVM in R}
\label{par:basic_svm_in_r}

Let's fire up R and create a dataset that almost looks like that data in
Figure~\ref{fig:svmdecision} so we can see how to classify it using the
SVM methods available in shikken.
 
<<initialize, results=hide, echo=TRUE, eval=TRUE>>=
library(MLplay)
library(shikken)

## Create two clas data
set.seed(123)
N <- 50
X1 <- matrix(c(rep(-2, N) - rnorm(N, 1), rnorm(N) + 2), ncol=2)
X2 <- matrix(c(rep( 2, N) + rnorm(N, 1), rnorm(N) - 2), ncol=2)

## Note that it's generally a good idea to scale your data
## (especially if its continuous) so we do that here.
X <- scale(rbind(X1, X2))
y <- rep(c(1, -1), each=N)
@

To use an SVM to clasify a dataset, shikken provides the \texttt{SVM}
method which accepts a dataset \texttt{x} of observations (rows), and
a label vector \texttt{y}. The \texttt{C} parameter is the same $C$
described in Formula~\ref{eqn:softmargin}, ie. the cost for non-zero
slack variables.

<<label=easyMargin,fig=TRUE,include=FALSE>>=
lsvm <- SVM(X, y, C=10)

## Does it accurately classify the data?
table(predict(lsvm, X), y)

plotDecisionSurface(lsvm, X, y)
@

The \texttt{plotDecisionSurface} function draws the data points
over a contour plot of the decision surface from the SVM. The decision
boundary is located at the $z=0$ contour. The points in red are the
examples that serve as the support vectors --- note how these examples
orient the decision boundary. As examples get further and further away
from the decision boundary, their decision values get increasingly higher.
When the SVM produces a high (absolute) decision value, you can think of
it as giving a high-confidence prediction of the class label. Positive
decision values belong to one class, negative values belong to the other.

\emph{What happens to the decision boundary and support vectors
when we decrease the value of $C$ to either $0.5$ or $0.1$.
Why does that happen?}

<<label=easyPlay,fig=FALSE>>=
lsvm <- SVM(X, y, C=0.5)
plotDecisionSurface(lsvm, X, y)

lsvm <- SVM(X, y, C=0.1)
plotDecisionSurface(lsvm, X, y)
@

As we all know, datasets aren't as well behaved as the toy example we've
tried so far. Let's imagine there was some noise in a measurement that
produced an outlier of our positively labeled data that looks a lot
closer to our negative data than the positive data.

<<label=easyOutlier,fig=FALSE>>=
X.out <- rbind(X, t(c(-2, -0.5)))
y.out <- c(y, 1)

lsvm <- SVM(X.out, y.out, C=10)
plotDecisionSurface(lsvm, X.out, y.out)

table(predict(lsvm, X.out), y.out)
@

The SVM is doing its job and classifying the data perfectly, but is this
desirable?

\emph{What should we do instead? Is the best classifier you can build the
one that always classifies your training data perfectly?}

<<label=easySoftMargin,echo=FALSE,include=FALSE,fig=TRUE>>=
lsvm <- SVM(X.out, y.out, C=1)
plotDecisionSurface(lsvm, X.out, y.out)
@

\begin{figure}[htbp]
  \centering  
  \mbox{\subfigure{\includegraphics[width=3in]{Rfigs/gen-easyMargin.pdf}}\quad
  \subfigure{\includegraphics[width=3in]{Rfigs/gen-easySoftMargin.pdf} }}
  \caption{
    Finding large margin hyperplane in well behaved data. A toy dataset is
    shown that is intuitively easy to classify. On the left, there isn't
    much left up to interpretation as to whether or not the SVM can finds
    the best separating hyperplane that has the largest margin. In the
    example on the right, imagine we observed an outlier in our set of
    positively labeled data. Trying to fit the hyperplane to classify that
    example will severely skew the decision boundary and rotate it to the
    right (as we saw in the examples). Setting $C = 0.5$ allows us to recover
    a classifier with a larger margin by allowing the SVM to misclassify the
    ``outlier''.
  }
  \label{fig:easymargin}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Kernels
\section{Kernels}
\label{sec:kernels}



Details of the optimization problem are summarized from


One particular drawback of SVMs is that even though they can
produce highly accurate classification machines, the interpretation of the
classifiers you can build can prove difficult. We will discuss some ways
to open the ``black box'' of your classifier in this vignette as well.

Other SVM tutorials:
What is a support vector machine~\cite{Noble:2006br}
SVMs and kernels for computational biology~\cite{BenHur:2008ec,}
%%to SVMs
%%   - Show primal
%%   - Show image of what it means to be a large margin classifier
%%   - Mention kernel trick
%%     This let's us measure differences between samples in different feature
%%     spaces more naturally.


%% ============================================================================
%% Kernels
\subsection{Kernels}
\label{sub:kernels}

Using kernels, a linear classifier can generate non-linear decision boundaries.
This is accomplished by mapping the data to another (typically higher dimension)
feature space using a function $\phi$. The discriminant function is then
transformed to:

\begin{align}
  f(x) = \left\langle {\bf w}, \phi ({\bf x}) \right\rangle + b
\label{eqn:phidisc}
\end{align}

$f(x)$ is linear in the feature space defined by the mapping function $\phi$,
but can look non-linear when projected back down to the original feature space.

A, perhaps non-obvious, advantage of measuring relationships between examples
through kernel functions is that we often do not have to map examples
into the higher dimensional feature space to calculate their similarity. This
project can be implicit, which can provide large memory and computational cost
gains. Refer to the polynomial kernel example below.

\begin{equation}
  f(x) = \sum_{i=1}^N \alpha_i y_i k({\bf x_i}, {\bf x}) + b
\label{eqn:kernelexp}
\end{equation}

%% ============================================================================
%% SVMs in R
\subsection{SVMs in R}
\label{sub:svms_in_r}

The are several interfaces to SVMs that already exist in \texttt{R}, namely
the \href{http://cran.r-project.org/web/packages/kernlab/}{kernlab}
and \href{http://cran.r-project.org/web/packages/e1071}{e1071} packages.
This tutorial, however, will introduce a new SVM package named
\href{http://lianos.github.com/shikken/}{shikken} which is a ``friendly''
wrapper to the
\href{http://www.shogun-toolbox.org/}{Shogun machine learning toolbox} --- a
\texttt{C++} library focussed on large scale kernel methods (especially
SVMs). We choose to use Shogun because it has a variety of string kernels
implemented in a memory and speed efficient manner, which will prove
useful in a variety of contexts that may arise when attempting to learn form
sequencing in your own research.



To extract the weight vector

\begin{equation}
  {\bf W} = \sum_{i=1}^N \alpha_i y_i {\bf x}_i
\label{eqn:wvector}
\end{equation}

@

<<label=svmPoly,fig=TRUE,include=FALSE>>=
## guts of makeCircle taken from plotrix package
set.seed(123)
Xcircle <- makeCircle(0, 0, 5, nv=100)
Xcircle <- cbind(Xcircle$x, Xcircle$y)
Xinside <- matrix(rnorm(200), ncol=2)

Xc <- rbind(Xcircle, Xinside)
yc <- rep(c(1, -1), each=100)

psvm <- SVM(Xc, yc, kernel="poly", C=10, degree=2)
table(predict(psvm, Xc), yc)

## Set wireframe=TRUE to see the decision surface in 3D
plotDecisionSurface(psvm, Xc, yc, wireframe=FALSE)
@

<<label=svmPoly3D,fig=TRUE,include=FALSE,echo=FALSE>>=
plotDecisionSurface(psvm, Xc, yc, wireframe=TRUE)
@

\begin{figure}[htbp]
  \centering  
  \mbox{\subfigure{\includegraphics[width=3in]{Rfigs/gen-svmPoly.pdf}}\quad
  \subfigure{\includegraphics[width=3in]{Rfigs/gen-svmPoly3D.pdf} }}
  \caption{Using a polynomial kernel to split data in a higher dimension}
  \label{fig:polykernel}
\end{figure}

Look at what the decision surface does when you increase the value for
\texttt{C} and set it to 10000. Can you intuitively describe why
that happens?

<<label=svmGaus,fig=TRUE,include=FALSE,echo=TRUE>>=
gsvm <- SVM(Xc, yc, kernel="gaussian", C=1000, width=1)
table(predict(psvm, Xc), yc)
plotDecisionSurface(gsvm, Xc, yc, wireframe=FALSE)
@

<<label=svmGaus3D,fig=TRUE,include=FALSE,echo=FALSE>>=
plotDecisionSurface(gsvm, Xc, yc, wireframe=TRUE)
@

\begin{figure}[htbp]
  \centering  
  \mbox{\subfigure{\includegraphics[width=3in]{Rfigs/gen-svmGaus.pdf}}\quad
  \subfigure{\includegraphics[width=3in]{Rfigs/gen-svmGaus3D.pdf} }}
  \caption{Using a gaussian kernel to split data in a higher dimension}
  \label{fig:polykernel}
\end{figure}



%% <<label=svmDecision>>=
%% ## Let's make 2D data from gaussians with different means
%% set.seed(123)
%%
%% Xgaus <- matrix(c(rnorm(2 * N, 1, 1), rnorm(2 * N, -1, 1)), ncol=2)
%% simplePlot(Xgaus, y)
%%
%% lsvm <- showSVM(Xgaus, y, kernel="linear", C=10)
%% lacc <- sum(predict(lsvm, Xgaus) == y) / length(y)
%% cat(sprintf("Linear svm accuracy: %.2f\n", lacc)
%%
%% gsvm <- showSVM(Xgaus, y, kernel="gaussian", C=10, width=1)
%% gacc <- sum(predict(gsvm, X) == y) / length(y)
%% cat(sprintf("Gaussian svm accuracy: %.2f\n", gacc))
%% @

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Classification on Strings
\section{Classification on strings using the spectrum kernel}
\label{sec:spectrum}

How do we project strings into a multi-dimensional space?

%% ============================================================================
%% Spectrum Kernel
\subsection{Spectrum Kernel}
\label{sub:spectrum_kernel}

The spectrum kernel.~\cite{Leslie:2002tx}
Mismatch string kerenls~\cite{Leslie:2004kt}

\begin{equation}
  k(x,x') = \sum_{u \in \Sigma^d} N(u, x) N(u, x')
\end{equation}

where $N(u,x)$ is the function that returns the number of occurrences of kmer
$u$ in the string $x$.

\emph{Exercise: Build your own spectrum kernel implementation}.
\emph{Exercise: How long do the kmers have to be to build a strong classifier?}

<<label=promoters>>=
data(promoters, package="shikken")
head(promoters)
pr.class <- values(promoters)$class
X.proms <- spectrumFeatures(promoters, degree=4)
@

TODO: Randomize promoters? reverse them, etc. does it still work?

But, what is the SVM actually saying? We can obviously classify a promoter from
background sequence, but how can we understand it?

%% ============================================================================
%% Feature Selection
\subsection{Feature Selection}
\label{sub:feature_selection}

TODO: Extract ${\bf W}$ and visualize with \texttt{worcloud} and recursive
feature elimination (RFE).

Learning interpretable SVMs for biological sequence classification~\cite{Ratsch:2006il}.
positional oligomer importance matrices (POIMs)~\cite{Sonnenburg:2008do}.


The elastic net can do similar~\cite{Zou:2005elastic}.
TODO: Show same example using \texttt{glmnet}.

%% ============================================================================
%% Characterize NOVA binding sites
\subsection{Characterize NOVA binding sites}
\label{sub:characterize_nova_binding_sites}

The RNA binding protein (RBP) known as \emph{NOVA-1} (and \emph{NOVA-2}) in higher eukaryotes, and pasilla (ps) in fly, has been previously implicated in playing a role in the regulation of alternative splicing.

Where does NOVA like to bind the pre-mRNA?

<<label=novamm9distro,fig=TRUE,include=TRUE>>=
data(annotated.genomes)
data(NOVA.mm9)

head(nova.peaks)

nova.summary <- data.table(as.data.frame(nova.peaks))[, {
  list(score=sum(score))
}, by="exon.anno"]

gg.angle <- opts(axis.text.x=theme_text(angle=-45, hjust=0, vjust=1))

g <- ggplot(nova.summary, aes(x=exon.anno, y=score, fill=exon.anno)) +
  geom_bar(stat="identity") + theme_bw() + gg.angle +
  opts(title="NOVA binding site regions in mm9")
print(g)
@

Try to use a spectrum kernel to identify preferred binding landscape of
\emph{NOVA-1/2} RNA binding protein.


%% ============================================================================
%% Training
\subsection{Training}
\label{sub:training}

%% ============================================================================
%% Testing
\subsection{Testing}
\label{sub:testing}


%% ============================================================================
%% Apply NOVA binding model to Drosophila homolog
\subsection{Apply NOVA binding model to Drosophila homolog}
\label{sub:apply_nova_binding_model_to_drosophila_homolog}


%% ============================================================================
%% Using multitask learning to combine datasets
\subsection{Using multitask learning to combine datasets}
\label{sub:using_multitask_learning_to_combine_datasets}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Higher resolution sequence kernels
\section{Higher resolution sequence kernels}
\label{sec:higher_resolution_sequence_kernels}

The spectrum kernel has no concept of position --- a kmer gets the same weight
if it appears at the beginning of a string or at the end.

%% ============================================================================
%% Weighted Degree Kernel
\subsection{Weighted Degree Kernel}
\label{sub:weighted_degree_kernel}

The \emph{weighted degree} (WD) kernel computes similarities between
sequences while taking positional information into account. The WD kernel
counts the exact co-occurrences of \emph{k}-mers at corresponding positions
of the two sequences being compared.

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=1]{figs/weighted-degree-kernel}
  \caption{
    Given two sequences $x_1$ and $x_2$ of equal length, the kernel
    computes a weighted sum of matching subsequences. Each matching
    subsequence makes a contribution $w_B$ depending on its length $B$,
    where longer matches contribute more significantly.}
  \label{fig:WDK}
\end{figure}

\begin{equation}
  k(x,x') = \sum_{k=1}^d \beta_k \sum_{i=1}^{l-k+1} \hbox{I}(u_{k,i}(x) = u_{k,i}(x'))
\end{equation}


%% ============================================================================
%% Weighted Degree Kernel with Shifts
\subsection{Weighted Degree Kernel with Shifts}
\label{sub:weighted_degree_kernel_with_shifts}

The weighted degree kernel with shifts (the WDS kernel) shifts the two
sequences against each other in order to allow for small positional variations
of sequence motifs. It is conceptually a combination of the spectrum and WD
kernels.

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=1]{figs/weighted-degree-kernel-with-shifts}
  \caption{
    Given two sequences $x_1$ and $x_2$ of equal length, the WDS kernel produces
    a weighted sum to which each match in the sequences makes a contribution
    $\gamma_{k,p}$ depending on its length $k$ and relative position $p$, where
    longer matches at the same position contribute more significantly.
  }
  \label{fig:WDKS}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Future things to explore
\section{Future things to explore}
\label{sec:future}

Multiple kernel learning.

Novel Machine Learning Methods for MHC Class I Binding Prediction
http://www.fml.tuebingen.mpg.de/raetsch/members/raetsch/bibliography/WTAKR2010
http://www.fml.tuebingen.mpg.de/raetsch/lectures/talk-multitask-recomb2010.pdf


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements
\section{Acknowledgements}
\label{sec:acknowledgements}

I had to pick a few brains in order to get this lab together and I'd like to
thank them here.

\emph{Chris Widmer} in Gunnar Raetch's lab for provided instrumental help with
many of the nuts-and-bolts questions I had about the shogun toolbox. He also
provided  materials from some of his previous talks about shogun as well as
multi-task learning that I adapted for some of the material in this tutorial.

\emph{Raphael Pelossof} and \emph{Alvaro Gonzalez} for useful conversations.
They are both post-docs in my advisor's lab and are always happy to share their
expertise.

Finnaly, I'd like to thank my advisor \emph{Christina Leslie} for always being
happy to shine her flash-light on just about everything.

\bibliography{MLplay}
\bibliographystyle{plain}


\section{Session Information}
<<sessionInfo>>=
sessionInfo()
@

\end{document}
