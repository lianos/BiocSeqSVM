%\VignetteIndexEntry{Maching learning from NGS data}
%\VignetteKeywords{tutorial}
%\VignettePackage{MLplay}
\documentclass[11pt]{article}

%% \usepackage{url}
\usepackage{hyperref}

\usepackage{colortbl}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{color}
\usepackage{xspace}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[boxed, linesnumbered]{algorithm2e}
\usepackage[
         colorlinks=true,
         linkcolor=blue,
         citecolor=blue,
         urlcolor=blue]
         {hyperref}
\usepackage{lscape}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Try to make things look purty
%% I like to use the fullpage!
\usepackage{fullpage}

%% Do not indent paragraphs
\usepackage[parfill]{parskip}

%% And fonts!
% For bold and small caps together
% http://stackoverflow.com/questions/699371/latex-small-caps-and-bold-face
\usepackage[T1]{fontenc}

%% Use Helvetica
\usepackage{times}
\usepackage[scaled=1]{helvet}     % PostScript font Helvetica for sans serif
\renewcommand{\rmdefault}{phv}    % Helvetica for roman type as well as sf
\renewcommand{\sfdefault}{phv}    % Helvetica for roman type as well as sf

% Inconsolata for monospaced fonts
% (fortunately this is distributed with the TeX-live distribution)
\usepackage{inconsolata}

%% End: Purty -----------------------------------------------------------------

\usepackage{Sweave}
\SweaveOpts{keep.source=TRUE}

% define new colors for use
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darkred}{rgb}{0.6,0.0,0}
\definecolor{lightbrown}{rgb}{1,0.9,0.8}
\definecolor{brown}{rgb}{0.6,0.3,0.3}
\definecolor{darkblue}{rgb}{0,0,0.8}
\definecolor{darkmagenta}{rgb}{0.5,0,0.5}

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}
\newcommand{\Rcode}[1]{{\texttt{#1}}}
\newcommand{\software}[1]{\textsf{#1}}
\newcommand{\R}{\software{R}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}

%% Either
% \lhead{}
% \chead{Machine Learning and NGS data}
% \rhead{}
% \lfoot{}
% \cfoot{}
% \rfoot{\thepage\ of \pageref{LastPage}}
% \renewcommand{\headrulewidth}{1pt}
% \renewcommand{\footrulewidth}{1pt}

%% Or
\fancyhead[RO,RE]{\slshape \leftmark}
% Using fancy headers sometimes lets figures, etc. bleed into the header, this
% trick has fixed it for me in the past
\setlength\headsep{25pt}

\title{Exploring the preferred binding landscape of an RNA binding protein using high throughput sequencing data and machine learning}

\author{
  Steve Lianoglou \\
  Memorial Sloan-Kettering Cancer Center \\
  \small{\texttt{lianos@cbio.mskcc.org}}
}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page

\maketitle

The goal of this tutorial is to provide a brief and intuitive introduction to
support vector machines (SVMs) and to ultimately show how they can be used to
explore new aspects of biology that we can now assay for using high throughput
sequencing (HTS) data.

The are several interfaces to SVMs that already exist in \code{R}, namely
the \href{http://cran.r-project.org/web/packages/kernlab/}{kernlab}
and \code{http://cran.r-project.org/web/packages/e1071}{e1071} packages.
This tutorial, however, will introduce a new SVM package named
\href{http://lianos.github.com/shikken/}{shikken} which is a ``friendly''
wrapper to the
\href{http://www.shogun-toolbox.org/}{Shogun machine learning toolbox} --- a
\code{C++} library focussed on large scale kernel methods (especially
SVMs). We choose to use Shogun because it has a variety of string kernels
implemented in a memory and speed efficient manner, which will prove
useful in a variety of contexts that may arise when attempting to learn form
sequencing in your own research.

\renewcommand{\baselinestretch}{.6}

\tableofcontents

\thispagestyle{empty}

\vspace{.2in}

\renewcommand{\baselinestretch}{1}

<<loadLibs, results=hide, echo=TRUE, eval=TRUE>>=
suppressPackageStartupMessages({
  library(Biostrings)
  library(shikken)
  library(fields)
})
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction to SVMs
%%   - Show primal
%%   - Show image of what it means to be a large margin classifier
%%   - Mention kernel trick
%%     This let's us measure differences between samples in different feature
%%     spaces more naturally.

<<label=easyClassificationProblem>>=
set.seed(123)
N <- 50

## easy to separate data
X1 <- matrix(c(rep(2, N) + rnorm(N, 0.5), 1 + rnorm(N)), ncol=2)
X2 <- matrix(c(rep(-2, N) - rnorm(N, 0.5), rnorm(N) -1), ncol=2)
X <- rbind(X1, X2)

y <- rep(c(1, -1), each=N)

simplePlot(X, y)

lsvm <- showSVM(X, y, kernel="linear", C=10, use.bias=FALSE)

gsvm <- showSVM(X, y, kernel="gaussian", C=10, width=1, use.bias=FALSE)

## What happens when it's not linearly separable?
X.harder <- rbind(X, matrix(c(0, 0), ncol=2))
y.harder <- c(y, 1)

simplePlot(X.harder, y.harder)

lsvm <- showSVM(X.harder, y.harder, kernel='linear', C=10, use.bias=TRUE)
## Calculate W
@

<<label=svmDecision>>=
## Let's make 2D data from gaussians with different means
set.seed(123)

##
Xgaus <- matrix(c(rnorm(2 * N, 1, 1), rnorm(2 * N, -1, 1)), ncol=2)
simplePlot(Xgaus, y)

lsvm <- showSVM(Xgaus, y, kernel="linear", C=10, use.bias=F)
cat(sprintf("Linear svm accuracy: %.2f\n",
            sum(predict(lsvm, Xgaus) == y) / length(y)))


gsvm <- showSVM(Xgaus, y, kernel="gaussian", C=10, width=1, use.bias=F)
cat(sprintf("Gaussian svm accuracy: %.2f\n", sum(predict(gsvm, X) == y) / length(y)))
@
%%   - Introduce string kernel as first/intuitive way to measure sequence
%%     similarity between two strings

<<label=promoters>>=
data(promoters, package="shikken")

@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Motivation
\section{Motivation}
\label{sec:motivation}
The RNA binding protein (RBP) known as \emph{NOVA-1} (and \emph{NOVA-2}) in higher eukaryotes, and pasilla (ps) in fly, has been previously implicated in playing a role in the regulation of alternative splicing.

<<label=novamm9distro>>=
data(annotated.genomes)
data(NOVA.mm9)
gd
nova.summary <- data.table(as.data.frame(nova.peaks))[, {
  list(score=sum(score))
}, by="exon.anno"]

gg.angle <- opts(axis.text.x=theme_text(angle=-45, hjust=0, vjust=1))

g <- ggplot(nova.summary, aes(x=exon.anno, y=score, fill=exon.anno)) +
  geom_bar(stat="identity") + theme_bw() + gg.angle +
  opts(title="NOVA binding site regions in mm9")
print(g)
@
%% ============================================================================
%% Caveat emptor
\subsection{Caveat emptor}
\label{sub:caveat_emptor}
This tutorial is not meant to serve as a general purpose introduction to
machine learning concepts. Other tutorials are available that you can find online
that can serve this purpose. In particular, the ML lab and slides prepared by
Vincent Carey for the
\href{http://www.bioconductor.org/help/course-materials/2011/CSAMA/}{CSAMA 2011 workshop}
are worth exploring.

We will be taking a deep dive into exploring how we can combine the types of high-resolution
data that different sequencing approaches provide with support vector machines (SVMs) to dig
around in the biology outlined in the motivation section above. Our goal is not to build a
100\% perfectly accurate classifier (although that would be nice!). We are more interested in
understanding something a bit more about biology using our machine learning models. We will
partially be relying on the functionality found in the Shogun machine learning toolbox, which is a
library written in \texttt{C++} with binding to several languages~\cite{Sonnenburg:2010tr}. The
downside to using Shogun from \texttt{R} is that it is not installable from \texttt{CRAN} and you
will find that its language bindings are highly un-idiomatic \texttt{R} style.
% subsection caveat_emptor

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Support vector machines
\section{Support Vector Machines}
\label{sec:support_vector_machines}

Support vector machines (SVMs) (cite vapnik) ...

%% ============================================================================
%% String Kernels
\subsection{String Kernels} % (fold)
\label{sub:string_kernels}

There are different ways to measure distance/similarity between strings.

%% ----------------------------------------------------------------------------
%% Spectrum Kernel
\subsubsection{Spectrum Kernel} % (fold)
\label{ssub:spectrum_kernel}
Cite Christina

\begin{equation}
  k(x,x') = \sum_{u \in \Sigma^d} N(u, x) N(u, x')
\end{equation}

where $N(u,x)$ is the function that returns the number of occurrences of kmer $u$ in the string $x$.

%% ----------------------------------------------------------------------------
%% Weighted Degree Kernel
\subsubsection{Weighted Degree Kernel}
\label{ssub:weighted_degree_kernel}

The \emph{weighted degree} (WD) kernel computes similarities between
sequences while taking positional information into account. The WD kernel
counts the exact co-occurrences of \emph{k}-mers at corresponding positions
of the two sequences being compared.

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=1]{figs/weighted-degree-kernel}
  \caption{
    Given two sequences $x_1$ and $x_2$ of equal length, the kernel
    computes a weighted sum of matching subsequences. Each matching
    subsequence makes a contribution $w_B$ depending on its length $B$,
    where longer matches contribute more significantly.}
  \label{fig:WDK}
\end{figure}

\begin{equation}
  k(x,x') = \sum_{k=1}^d \beta_k \sum_{i=1}^{l-k+1} \hbox{I}(u_{k,i}(x) = u_{k,i}(x'))
\end{equation}


%% ----------------------------------------------------------------------------
%% Weighted Degree Kernel with Shifts
\subsubsection{Weighted Degree Kernel with Shifts}
\label{ssub:weighted_degree_kernel_with_shifts}

The weighted degree kernel with shifts (the WDS kernel) shifts the two
sequences against each other in order to allow for small positional variations
of sequence motifs. It is conceptually a combination of the spectrum and WD
kernels.

\begin{figure}[htbp]
  \centering
    \includegraphics[scale=1]{figs/weighted-degree-kernel-with-shifts}
  \caption{
    Given two sequences $x_1$ and $x_2$ of equal length, the WDS kernel produces
    a weighted sum to which each match in the sequences makes a contribution
    $\gamma_{k,p}$ depending on its length $k$ and relative position $p$, where
    longer matches at the same position contribute more significantly.
  }
  \label{fig:WDKS}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Learning preferred binding landscapes
\section{Learning preferred binding landscapes}
\label{sec:learning_preferred_binding_landscapes}


%% ============================================================================
%% Feature selection
\subsection{Feature selection}
\label{sub:feature_selection}


%% ============================================================================
%% Training
\subsection{Training}
\label{sub:training}



%% ============================================================================
%% Testing
\subsection{Testing}
\label{sub:testing}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Applying model on novel organism
\section{Applying model on novel organism}
\label{sec:applying_model_on_novel_organism}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Unsupervised learning with NFM
\section{Unsupervised learning with Nonnegative Matrix Factorization}
\label{sec:unsupervised_learning_with_nonnegative_matrix_factorization}
Non-negative matrix factorization


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Future things to explore
\section{Future things to explore}
\label{sec:future}
Novel Machine Learning Methods for MHC Class I Binding Prediction
http://www.fml.tuebingen.mpg.de/raetsch/members/raetsch/bibliography/WTAKR2010
http://www.fml.tuebingen.mpg.de/raetsch/lectures/talk-multitask-recomb2010.pdf


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements
\section{Acknowledgements}
\label{sec:acknowledgements}

I had to pick a few brains in order to get this lab together and I'd like to
thank them here.

\emph{Chris Widmer} in Gunnar Raetch's lab for provided instrumental help with
many of the nuts-and-bolts questions I had about the shogun toolbox. He also
provided  materials from some of his previous talks about shogun as well as
multi-task learning that I adapted for some of the material in this tutorial.

\emph{Raphael Pelossof} and \emph{Alvaro Gonzalez} for useful conversations.
They are both post-docs in my advisor's lab and are always happy to share their
expertise.

Finnaly, I'd like to thank my advisor \emph{Christina Leslie} for always being
happy to shine her flash-light on just about everything.

\bibliography{MLplay}
\bibliographystyle{plain}


\section{Session Information}
<<sessionInfo>>=
sessionInfo()
@

\end{document}
